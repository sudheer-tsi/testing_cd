name: Fabric CI/CD - Dev to Prod Deployment

# Trigger: Runs when code is pushed to cd-dev branch
on:
  push:
    branches:
      - cd-prod
  workflow_dispatch:  # Allows manual trigger from GitHub UI

# Environment variables - shared across all jobs
env:
  FABRIC_API_URL: "https://api.fabric.microsoft.com/v1"
  
jobs:
  deploy-to-production:
    runs-on: ubuntu-latest
    
    steps:
      # Step 1: Checkout the repository code
      - name: Checkout Repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Get full history for comparison
      
      # Step 2: Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      # Step 3: Install required Python packages
      - name: Install Dependencies
        run: |
          pip install pyyaml requests jsonpath-ng
      
      # Step 4: Create deployment configuration
      - name: Create parameters.yml
        run: |
          cat > parameters.yml << 'EOF'
          # Environment-specific configurations
          environments:
            dev:
              workspace_id: "6e6de13a-f2dc-4623-b3ac-431ec6d345f1"  # Your dev workspace
              workspace_name: "Fabric_CD_Test_Dev"
            prod:
              workspace_id: "${{ secrets.PROD_WORKSPACE_ID }}"
              workspace_name: "${{ secrets.PROD_WORKSPACE_NAME }}"
          
          # Artifact ID mappings
          find_replace:
            # Raw Lakehouse replacements
            - find_value: "your-dev-raw-lakehouse-id"
              replace_value:
                dev: "your-dev-raw-lakehouse-id"
                prod: "${{ secrets.PROD_RAW_LAKEHOUSE_ID }}"
              item_type: "Notebook"
              file_path:
                - "01_bronze/edp_dev_raw.Lakehouse/**/*.py"
            
            # Stage Lakehouse replacements  
            - find_value: "your-dev-stg-lakehouse-id"
              replace_value:
                dev: "your-dev-stg-lakehouse-id"
                prod: "${{ secrets.PROD_STG_LAKEHOUSE_ID }}"
              item_type: "Notebook"
              file_path:
                - "nb_consolidated_historic_inventory Copy.Notebook/**/*.py"
          
          key_value_replace:
            # Pipeline artifact references
            - find_key: "$.properties.activities[*].typeProperties.sink.datasetSettings.typeProperties.artifactId"
              replace_value:
                dev: "your-dev-pipeline-artifact-id"
                prod: "${{ secrets.PROD_PIPELINE_ARTIFACT_ID }}"
              item_type: "DataPipeline"
              item_name: "pl_ingestion"
              file_path:
                - "pl_ingestion.DataPipeline/pipeline-content.json"
          EOF
      
      # Step 5: Create Python deployment script
      - name: Create Deployment Script
        run: |
          cat > deploy.py << 'EOF'
          import os
          import yaml
          import json
          import requests
          from pathlib import Path
          from jsonpath_ng import parse
          import base64
          
          class FabricDeployer:
              def __init__(self, environment):
                  self.environment = environment
                  self.params = self.load_parameters()
                  self.access_token = self.get_access_token()
                  
              def load_parameters(self):
                  with open('parameters.yml', 'r') as f:
                      return yaml.safe_load(f)
              
              def get_access_token(self):
                  """Get Azure AD access token for Fabric API"""
                  tenant_id = os.environ.get('AZURE_TENANT_ID')
                  client_id = os.environ.get('AZURE_CLIENT_ID')
                  client_secret = os.environ.get('AZURE_CLIENT_SECRET')
                  
                  url = f"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token"
                  data = {
                      'grant_type': 'client_credentials',
                      'client_id': client_id,
                      'client_secret': client_secret,
                      'scope': 'https://analysis.windows.net/powerbi/api/.default'
                  }
                  
                  response = requests.post(url, data=data)
                  response.raise_for_status()
                  return response.json()['access_token']
              
              def process_find_replace(self):
                  """Process simple string replacements"""
                  print("Processing find_replace rules...")
                  
                  for rule in self.params.get('find_replace', []):
                      find_val = rule['find_value']
                      replace_val = rule['replace_value'][self.environment]
                      
                      for file_pattern in rule['file_path']:
                          for file_path in Path('.').glob(file_pattern):
                              if file_path.is_file():
                                  print(f"  Processing: {file_path}")
                                  content = file_path.read_text(encoding='utf-8')
                                  
                                  if find_val in content:
                                      content = content.replace(find_val, replace_val)
                                      file_path.write_text(content, encoding='utf-8')
                                      print(f"    ✓ Replaced {find_val[:8]}... with {replace_val[:8]}...")
              
              def process_key_value_replace(self):
                  """Process JSON path-based replacements"""
                  print("\nProcessing key_value_replace rules...")
                  
                  for rule in self.params.get('key_value_replace', []):
                      jsonpath_expr = parse(rule['find_key'])
                      replace_val = rule['replace_value'][self.environment]
                      
                      for file_pattern in rule['file_path']:
                          for file_path in Path('.').glob(file_pattern):
                              if file_path.is_file() and file_path.suffix == '.json':
                                  print(f"  Processing: {file_path}")
                                  
                                  with open(file_path, 'r', encoding='utf-8') as f:
                                      data = json.load(f)
                                  
                                  matches = jsonpath_expr.find(data)
                                  if matches:
                                      for match in matches:
                                          match.full_path.update(data, replace_val)
                                      
                                      with open(file_path, 'w', encoding='utf-8') as f:
                                          json.dump(data, f, indent=2)
                                      print(f"    ✓ Updated {len(matches)} JSON paths")
              
              def deploy_to_fabric(self):
                  """Deploy artifacts to Fabric workspace"""
                  print("\nDeploying to Fabric workspace...")
                  
                  workspace_id = self.params['environments'][self.environment]['workspace_id']
                  headers = {
                      'Authorization': f'Bearer {self.access_token}',
                      'Content-Type': 'application/json'
                  }
                  
                  # Deploy notebooks
                  self.deploy_notebooks(workspace_id, headers)
                  
                  # Deploy pipelines
                  self.deploy_pipelines(workspace_id, headers)
                  
                  print("\n✓ Deployment completed successfully!")
              
              def deploy_notebooks(self, workspace_id, headers):
                  """Deploy notebook artifacts"""
                  print("\n  Deploying notebooks...")
                  
                  for notebook_path in Path('.').glob('**/*.Notebook'):
                      if notebook_path.is_dir():
                          notebook_name = notebook_path.name.replace('.Notebook', '')
                          content_file = notebook_path / 'notebook-content.py'
                          
                          if content_file.exists():
                              print(f"    Deploying: {notebook_name}")
                              
                              with open(content_file, 'r', encoding='utf-8') as f:
                                  content = f.read()
                              
                              # Create or update notebook via API
                              url = f"{os.environ['FABRIC_API_URL']}/workspaces/{workspace_id}/notebooks"
                              payload = {
                                  'displayName': notebook_name,
                                  'definition': {
                                      'format': 'ipynb',
                                      'parts': [
                                          {
                                              'path': 'notebook-content.py',
                                              'payload': base64.b64encode(content.encode()).decode(),
                                              'payloadType': 'InlineBase64'
                                          }
                                      ]
                                  }
                              }
                              
                              response = requests.post(url, headers=headers, json=payload)
                              if response.status_code in [200, 201]:
                                  print(f"      ✓ {notebook_name} deployed")
                              else:
                                  print(f"      ✗ Failed: {response.status_code} - {response.text}")
              
              def deploy_pipelines(self, workspace_id, headers):
                  """Deploy pipeline artifacts"""
                  print("\n  Deploying pipelines...")
                  
                  for pipeline_path in Path('.').glob('**/*.DataPipeline'):
                      if pipeline_path.is_dir():
                          pipeline_name = pipeline_path.name.replace('.DataPipeline', '')
                          content_file = pipeline_path / 'pipeline-content.json'
                          
                          if content_file.exists():
                              print(f"    Deploying: {pipeline_name}")
                              
                              with open(content_file, 'r', encoding='utf-8') as f:
                                  content = json.load(f)
                              
                              # Create or update pipeline via API
                              url = f"{os.environ['FABRIC_API_URL']}/workspaces/{workspace_id}/dataPipelines"
                              payload = {
                                  'displayName': pipeline_name,
                                  'definition': content
                              }
                              
                              response = requests.post(url, headers=headers, json=payload)
                              if response.status_code in [200, 201]:
                                  print(f"      ✓ {pipeline_name} deployed")
                              else:
                                  print(f"      ✗ Failed: {response.status_code} - {response.text}")
              
              def run(self):
                  """Execute full deployment process"""
                  print(f"Starting deployment to {self.environment.upper()} environment\n")
                  print("=" * 60)
                  
                  self.process_find_replace()
                  self.process_key_value_replace()
                  self.deploy_to_fabric()
                  
                  print("=" * 60)
          
          if __name__ == "__main__":
              environment = os.environ.get('TARGET_ENVIRONMENT', 'prod')
              deployer = FabricDeployer(environment)
              deployer.run()
          EOF
      
      # Step 6: Run the deployment
      - name: Deploy to Production
        env:
          TARGET_ENVIRONMENT: prod
          AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
          AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          AZURE_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
        run: |
          python deploy.py
      
      # Step 7: Create deployment summary
      - name: Deployment Summary
        if: always()
        run: |
          echo "## Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** Production" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "**Triggered by:** ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
